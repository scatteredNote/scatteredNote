[{"grab":"## Edit/Create/modify Warehouse via sql worksheet interface","views":"```sql\nCREATE WAREHOUSE \"TEST_WH\" \nWITH WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 600 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 2 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\nAlter warehouse\n\n```sql\nALTER WAREHOUSE \"TEST_WH\" \nSET WAREHOUSE_SIZE = 'SMALL' \nAUTO_SUSPEND = 1200 \nAUTO_RESUME = TRUE \nMIN_CLUSTER_COUNT = 1 \nMAX_CLUSTER_COUNT = 1 \nSCALING_POLICY = 'STANDARD' \nCOMMENT = ' '\n```\n\n### View warehouses\n\n```sql\nSHOW WAREHOUSES\n```\n\n```sql\nALTER WAREHOUSE TEST_WH SUSPEND\n```\n\n```sql\n\nALTER WAREHOUSE \"TEST_WH\" RESUME If SUSPENDED\n\n```\n\n### Drop warehouse\n\n```sql\n\nDROP WAREHOUSE \"TEST_WH\"\n```","tags":["snowflakes"]},{"grab":"## Creating DB\n\nDatabase can be created by going to  the `Data` section and click on `+ Database`.\n\nand via the ui you can clone local db and drop any db.\n\nAlso this can be done via the worksheet sql","views":"```sql\ncreate DATABASE \"TEST_DB_2\"\n\nSHOW DATABASES\n\nCREATE DATABASE TEST_DB_3 CLONE \"TEST_DB_2\"\n\nDROP DATABASE \"TEST_DB_2\"\n```","tags":["snowflakes"]},{"grab":"## creatin schema for database via sql worksheets","views":"```sql\nCREATE SCHEMA \"TEST_DB\".\"TEST_SCHEMA\"\n\n```\n\nwhere `TEST_SCHEMA` is schema created under `TEST_DB`\n\nedit the schema name\n\n```sql\n\nALTER SCHEMA \"TEST_DB\".\"TEST_SCHEMA\" RENAME TO \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\n```\n\n```sql\n\nSHOW SCHEMAS\n\nCREATE SCHEMA \"TEST_DB\".\"TEST2\" CLONE \"TEST_DB\".\"TEST_SCHEMA_RENAME\"\n\nDROP SCHEMA \"TEST_DB\".\"TEST2\"\n```","tags":["snowflakes"]},{"grab":"assign warehouse creation to a role","views":"```sql\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE kafka_connector_role_1;\n```","tags":[]},{"grab":"Enable kafka to write to snowflake\n","views":"create a db and schema\n\ngo to worksheet and do the following\n\n* Add public key to the user on snowflake\n\n```sql\nALTER USER STEVEONI set rsa_public_key_2=\"\n```\ncreate a role and give it access to table schema , table, stage and pipe\n\n```sql\nUSE ROLE accountadmin;\n\n-- Create a Snowflake role with the privileges to work with the connector.\nCREATE ROLE kafka_connector_role_1;\n\nCREATE DATABASE kafka_db\n-- Grant privileges on the database.\nGRANT USAGE ON DATABASE kafka_db TO ROLE kafka_connector_role_1;\n\n-- Grant privileges on the schema.\nGRANT USAGE ON SCHEMA KAFKA_SCHEMA TO ROLE kafka_connector_role_1;\nGRANT CREATE TABLE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE STAGE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\nGRANT CREATE PIPE ON SCHEMA kafka_schema TO ROLE kafka_connector_role_1;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n\nGRANT ROLE kafka_connector_role_1 TO USER STEVEONI;\n```\n\nGive permission to the new role to create a warehouse.\n\nthen create a warehoues\n\n","tags":[]},{"grab":"## Snowflakes object and funtionalities","views":" overview of various Snowflake concepts, their purpose, and examples:\n\n1. **Views**:\n   - **Overview**: Views are virtual tables created by executing a SQL query on one or more base tables. They are not materialized and don't store data themselves but provide a convenient way to access and transform data.\n   - **Purpose**: To simplify complex queries, control access to data, and abstract underlying table structures.\n   - **Example**:\n     ```sql\n     CREATE VIEW sales_2022 AS\n     SELECT product_name, SUM(sales_amount) AS total_sales\n     FROM sales\n     WHERE sales_date BETWEEN '2022-01-01' AND '2022-12-31'\n     GROUP BY product_name;\n     ```\n\n2. **Stage**:\n   - **Overview**: A stage is an intermediate storage location used to load and unload data between Snowflake and external storage (e.g., AWS S3, Azure Blob Storage).\n   - **Purpose**: Efficiently move data in and out of Snowflake while leveraging cloud-based storage.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE STAGE my_stage\n     URL = 's3://my-bucket/my-path/'\n     CREDENTIALS = (AWS_KEY_ID = 'your-key-id' AWS_SECRET_KEY = 'your-secret-key');\n     ```\n\n3. **Pipe**:\n   - **Overview**: A pipe is a Snowflake object used to automate the ingestion of data from an external stage into a Snowflake table. It's typically associated with Snowflake Streams for continuous data loading.\n   - **Purpose**: Automate data loading processes and provide a consistent way to ingest data.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE PIPE my_pipe\n     AUTO_INGEST = TRUE\n     AS\n     COPY INTO my_table FROM '@my_stage/'\n     FILE_FORMAT = (TYPE = CSV);\n     ```\n\n4. **Storage Integration**:\n   - **Overview**: A storage integration allows Snowflake to access external cloud storage services securely. It includes credentials and configurations.\n   - **Purpose**: Facilitate seamless data exchange between Snowflake and cloud storage providers.\n   - **Example**:\n     ```sql\n     CREATE STORAGE INTEGRATION my_integration\n     TYPE = EXTERNAL_STAGE\n     ENABLED = TRUE\n     STORAGE_PROVIDER = S3\n     CREDENTIALS = (AWS_ROLE = 'my-iam-role');\n     ```\n\n5. **File Format**:\n   - **Overview**: A file format defines how data is stored and organized within files stored in external stages. It includes details like file type, compression, and delimiter.\n   - **Purpose**: Specify how data should be read from or written to external files.\n   - **Example**:\n     ```sql\n     CREATE OR REPLACE FILE FORMAT my_format\n     TYPE = CSV\n     FIELD_OPTIONALLY_ENCLOSED_BY = ''\n     SKIP_HEADER = 1;\n     ```\n\n6. **Sequence**:\n   - **Overview**: A sequence is an object that generates unique numeric values, often used for creating surrogate keys.\n   - **Purpose**: Generate unique identifiers for rows in a table.\n   - **Example**:\n     ```sql\n     CREATE SEQUENCE my_sequence\n     START = 1\n     INCREMENT = 1;\n     ```\n\n7. **Stream**:\n   - **Overview**: A stream is a change tracking mechanism in Snowflake. It captures changes made to a table and can be used for CDC (Change Data Capture) and data replication.\n   - **Purpose**: Track and propagate changes in a table.\n   - **Example**:\n     ```sql\n     CREATE STREAM my_stream\n     ON TABLE my_table;\n     ```\n\n8. **Task**:\n   - **Overview**: A task is a scheduled or one-time execution of SQL statements. It can be used for automation, ETL, and routine maintenance.\n   - **Purpose**: Automate SQL execution on a schedule or event.\n   - **Example**:\n     ```sql\n     CREATE TASK my_task\n     WAREHOUSE = my_warehouse\n     SCHEDULE = '5 minute'\n     AS\n     INSERT INTO my_summary_table\n     SELECT ...\n     ```\n\n9. **Function** and **Procedure**:\n   - **Overview**: Functions are reusable SQL code blocks that return a value, while procedures are reusable code blocks that perform an action.\n   - **Purpose**: Encapsulate logic and promote code reuse.\n   - **Example (Function)**:\n     ```sql\n     CREATE OR REPLACE FUNCTION add_numbers(x INT, y INT)\n     RETURNS INT\n     AS\n     'x + y';\n     ```\n\n10. **Dynamic Table**:\n    - **Overview**: Dynamic tables are temporary tables created and dropped during a session. They're used for intermediate data processing.\n    - **Purpose**: Temporarily store data for complex operations without affecting permanent tables.\n    - **Example**:\n      ```sql\n      CREATE TEMPORARY TABLE temp_table AS\n      SELECT ...\n      ```\n\nThese are fundamental Snowflake concepts that cover various aspects of data storage, processing, and automation within the Snowflake data warehouse.","tags":["snowflakes"]},{"grab":"## staging using Amazon s3","views":"\n**Step-by-Step Operation:**\n\n**Step 1: Set Up Snowflake and Amazon S3**\n\nEnsure you have a Snowflake account and an Amazon S3 bucket set up. Replace placeholders in angle brackets (<>) with your actual values.\n\n**Step 2: Create a Stage in Snowflake**\n\nIn Snowflake, create an internal stage to specify the S3 bucket as the staging location:\n\n```sql\n-- Create a stage\nCREATE OR REPLACE STAGE my_stage\nURL = 's3://<your-s3-bucket>/<path-to-folder>'\nCREDENTIALS = (\n  AWS_KEY_ID = '<your-aws-access-key-id>'\n  AWS_SECRET_KEY = '<your-aws-secret-access-key>'\n);\n```\n\nThis step establishes a connection to your S3 bucket.\n\n**Step 3: Copy Data to Snowflake Stage**\n\nYou can copy data from your S3 bucket to the Snowflake stage using the `COPY INTO` command. This command specifies the source and target:\n\n```sql\n-- Copy data into Snowflake stage\nCOPY INTO my_stage\nFROM 's3://<your-s3-bucket>/<source-path>'\nFILES = ('file1.csv', 'file2.csv')\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command copies data from the specified S3 path and files into your Snowflake stage.\n\n**Step 4: Perform Data Operations**\n\nYou can now perform various data operations on the staged data using SQL commands:\n\n```sql\n-- Example: Create a table and load data from the stage\nCREATE OR REPLACE TABLE my_table AS\nSELECT *\nFROM @my_stage/file1.csv;\n```\n\nHere, we create a new table in Snowflake and load data from the staged CSV file.\n\n**Step 5: Copy Data Back to S3 (Optional)**\n\nAfter processing the data, if you need to write the results back to S3, you can use the `COPY INTO` command again to copy data from Snowflake to S3:\n\n```sql\n-- Copy data from Snowflake to S3\nCOPY INTO 's3://<your-s3-bucket>/<destination-path>'\nFROM my_table\nFILE_FORMAT = (TYPE = CSV);\n```\n\nThis command exports data from Snowflake to the specified S3 location.\n\n**Step 6: Clean Up (Optional)**\n\nYou can optionally clean up the staged data and stage by dropping them:\n\n```sql\n-- Drop the stage\nDROP STAGE my_stage;\n\n-- Drop the table if it's no longer needed\nDROP TABLE my_table;\n```\nStaging allows you to efficiently transfer and process data between Snowflake and cloud storage services like S3 while maintaining data integrity and security.","tags":["snowflakes"]},{"grab":"## internal storage","views":"\n**Overview:**\n\n- Internal stages are automatically managed by Snowflake and don't require separate storage account credentials.\n- They are temporary and used within Snowflake for various operations like data loading, unloading, and transformations.\n- Internal stages are often used for storing intermediate data during ETL (Extract, Transform, Load) processes within Snowflake.\n\n**Creating an Internal Stage:**\n\nYou can create an internal stage using the following SQL query:\n\n```sql\n-- Create an internal Snowflake stage\nCREATE OR REPLACE STAGE my_internal_stage;\n```\n\n**Using an Internal Stage:**\n\nOnce created, you can use the internal stage in various Snowflake operations. For example, you can copy data from a Snowflake table into an internal stage or vice versa:\n\n```sql\n-- Copy data from Snowflake table to the internal stage\nCOPY INTO @my_internal_stage/<your-file-prefix>\nFROM my_snowflake_table\nFILE_FORMAT = (TYPE = CSV);\n\n-- Copy data from the internal stage to a Snowflake table\nCOPY INTO my_snowflake_table\nFROM @my_internal_stage/<your-file-prefix>\nFILE_FORMAT = (TYPE = CSV);\n```\n\nIn this example, `my_internal_stage` is the name of the internal Snowflake stage. You can use it as a source or destination for data transfer operations.\n\n**Use Cases:**\n\n- Internal stages are helpful for intermediate data storage during complex data transformations.\n- They can be used as temporary storage when processing data with Snowflake functions, such as transformations using stored procedures or tasks.\n- Internal stages are not exposed outside of Snowflake and are mainly used for managing data within the Snowflake ecosystem.\n","tags":["snowflakes"]},{"grab":"## Pipe\n\nIs use to ingest data from table to another in realtime. as a table is updated e.g stage it automatically copy the data over to the other table","views":"e.g\n\nLet's say you have an external streaming service that produces JSON data, and you want to ingest this data into a Snowflake table.\n\n```sql\n-- Create a pipe to ingest data from an external source (e.g., an S3 bucket)\nCREATE OR REPLACE PIPE my_pipe\n  AUTO_INGEST = TRUE\n  INTEGRATION = my_storage_integration\n  AS\n  COPY INTO my_snowflake_table\n  FROM @my_external_stage\n  FILE_FORMAT = (TYPE = JSON);\n\n```\n\nfor Internal stage:\n\n```sql\n-- Create an internal stage (if it doesn't exist)\nCREATE OR REPLACE STAGE my_internal_stage;\n\n-- Create a pipe to ingest data from an internal stage\nCREATE OR REPLACE PIPE my_pipe_internal_stage\n  AUTO_INGEST = TRUE\n  AS\n  COPY INTO my_snowflake_table\n  FROM @my_internal_stage\n  FILE_FORMAT = (TYPE = JSON);\n\n```","tags":["snowflakes"]},{"grab":"## Notification\n\nsnowflakes can send notification for data ingestion and the likes. this helps to monitor services","views":"```sql\n-- Define a notification integration for email notifications\nCREATE NOTIFICATION INTEGRATION my_email_integration\n  TYPE = EMAIL\n  NOTIFICATION_PROVIDER = 'smtp'\n  ENABLED = TRUE\n  EMAIL_SUBJECT = 'Data Loading Notification'\n  EMAIL_SENDER_ADDRESS = 'your_email@example.com'\n  EMAIL_RECIPIENT_ADDRESS = 'recipient@example.com';\n\n-- Associate the notification integration with a pipe\nCREATE OR REPLACE PIPE my_pipe\n  AUTO_INGEST = TRUE\n  INTEGRATION = my_email_integration\n  ...other pipe settings...;\n\n```\n\nThe type can either be `EMAIL | QUEUE`","tags":["snowflakes"]},{"grab":"## Storage Integration\n\nGood for securly managing external data wrt snoweflakes.\n\nE.g if we are to move data from  amazon s3 to snowflakes and want to mange that via staging.\n\nwe can create a storage integration with credentials set and no need to always set the credentials incase we are to create another staging","views":"1. Define a storage integration:\n```sql\n-- Create a Storage Integration for Amazon S3\nCREATE STORAGE INTEGRATION my_s3_integration\n  TYPE = EXTERNAL_STAGE\n  ENABLED = TRUE\n  STORAGE_PROVIDER = S3\n  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/SnowflakeRole'\n  STORAGE_AWS_EXTERNAL_ID = 'snowflake-external-id';\n\n```\n\n2. Access control:\n```sql\n-- Grant USAGE privilege on the Storage Integration\nGRANT USAGE ON INTEGRATION my_s3_integration TO ROLE my_role;\n```\n\n3. Using a storage integration:\n```sql\n-- Create an external stage using the Storage Integration\nCREATE OR REPLACE STAGE my_external_stage\n  URL = 's3://my-bucket/data/'\n  STORAGE_INTEGRATION = my_s3_integration;\n\n-- Copy data from the external stage to a Snowflake table\nCOPY INTO my_snowflake_table\n  FROM @my_external_stage/data\n  FILES = ('file1.csv', 'file2.csv');\n\n```","tags":["snowflakes"]},{"grab":"## File format\n\nis use to determin how file is read into snowflakes or copied out of snowflakes\n\n","views":"```sql\nCREATE OR REPLACE FILE FORMAT my_csv_format\nTYPE = 'CSV'\nFIELD_OPTIONALLY_ENCLOSED_BY = ''\nSKIP_HEADER = 1;\n```","tags":["snowflakes"]},{"grab":"## sequence\n\nUse to create sequential unique numbers","views":"```sql\nCREATE OR REPLACE SEQUENCE seq_01 START = 1 INCREMENT = 1;\nCREATE OR REPLACE TABLE sequence_test_table (i INTEGER);\n```\n\nrun the select query on seq_01 to generate sequential numbers\n\n```sql\nSELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       1 |\n+---------+\n```\n2nd time\n\n```sql\nSELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       2 |\n+---------+\n```\n\ninsert the dequence number into a table\n\n```sql\nINSERT INTO sequence_test_table (i) VALUES (seq_01.nextval);\n\nSELECT i FROM sequence_test_table;\n+---+\n| I |\n|---|\n| 3 |\n+---+\n```\n ","tags":["snowflakes"]},{"grab":"## stream\n\nAllows us to capture and track changes (real time) to tables.","views":"```sql\nCREATE STREAM my_stream ON TABLE my_table;\n\n```\n\ntracks every changes to table `my_table`\n\n\nto see the streams \n\n```sql\nselect * from my_stream\n\n+----+--------+-----+-----------------+-------------------+------------------------------------------+\n| ID | NAME   | FEE | METADATA$ACTION | METADATA$ISUPDATE | METADATA$ROW_ID                          |\n|----+--------+-----+-----------------+-------------------+------------------------------------------|\n|  1 | Joe    |  90 | INSERT          | False             | 957e84b34ef0f3d957470e02bddccb027810892c |\n|  2 | Jane   |  90 | INSERT          | False             | b00168a4edb9fb399dd5cc015e5f78cbea158956 |\n|  3 | George |  90 | INSERT          | False             | 75206259362a7c89126b7cb039371a39d821f76a |\n|  4 | Betty  |   0 | INSERT          | False             | 9b225bc2612d5e57b775feea01dd04a32ce2ad18 |\n|  5 | Sally  |   0 | INSERT          | False             | 5a68f6296c975980fbbc569ce01033c192168eca |\n+----+--------+-----+-----------------+-------------------+------------------------------------------+\n\n```","tags":["snowflakes"]},{"grab":"## Task\n\nthis similar to cron job, help to schedule a time to which a job (sql statement ) should be executed.\n\ne.g\n\nLets schedule an task to copy file from an s3 bucket into snowflakes at a particular time","views":"**define external stage**\n\n```sql\n-- Create an external stage pointing to your external data source (e.g., an S3 bucket)\nCREATE OR REPLACE STAGE my_external_stage\n  URL = 's3://your-external-bucket/data/'\n  CREDENTIALS = (\n    AWS_KEY_ID = 'your-aws-access-key-id',\n    AWS_SECRET_KEY = 'your-aws-secret-access-key'\n  );\n\n```\n\n**create snowflake table**\n\n```sql\n-- Create a table to store the external data\nCREATE OR REPLACE TABLE my_target_table (\n  column1 STRING,\n  column2 INT,\n  -- Define columns as needed\n);\n\n```\n\n\n\n**create a task**\n\n```sql\nCREATE OR REPLACE TASK my_daily_load_task\n  WAREHOUSE = my_warehouse\n  SCHEDULE = 'DAILY 01:00' -- Run daily at 01:00 AM\n  COMMENT = 'Task to load data from external source'\nAS\n  -- Define the SQL statement to load data into the target table from the external stage\n  BEGIN\n    COPY INTO my_target_table FROM @my_external_stage/\n    FILE_FORMAT = (TYPE = CSV); -- Specify the file format as needed\n  END;\n```","tags":["snowflakes"]},{"grab":"## user defined function and procedure\n\nFunction and procedure help create utlity that can be call when needed.\n\nAlso can help to perform more complex action not pre-defined in sql, by reaching out to enviroment like javascript and  python.","views":"**Function**\n\n```sql\n\n-- Create a scalar function that calculates the square of a number\nCREATE OR REPLACE FUNCTION calculate_square(input_num FLOAT)\n  RETURNS FLOAT\n  LANGUAGE SQL\n  AS '\n    SELECT input_num * input_num;\n  ';\n\n-- Use the function in a query\nSELECT calculate_square(5); -- Returns 25.0\n```\n\n**Procedure**\n\n```sql\n-- Create a procedure that inserts a new record into a table\nCREATE OR REPLACE PROCEDURE insert_employee(emp_id INT, emp_name STRING)\n  RETURNS STRING\n  LANGUAGE SQL\n  EXECUTE AS CALLER\n  AS '\n    INSERT INTO employees (employee_id, employee_name)\n    VALUES (:emp_id, :emp_name);\n    RETURN \\'Record inserted successfully.\\';\n  ';\n\n-- Call the procedure\nCALL insert_employee(101, 'John Doe');\n\n```\n\n**Procedure advance**\n\n```sql\n-- Create a procedure that calculates the average salary of employees by department\nCREATE OR REPLACE PROCEDURE calculate_avg_salary_by_dept()\n  RETURNS TABLE (department STRING, avg_salary FLOAT)\n  LANGUAGE SQL\n  EXECUTE AS CALLER\n  AS '\n    SELECT department, AVG(salary) AS avg_salary\n    FROM employees\n    GROUP BY department;\n  ';\n\n-- Call the procedure and retrieve the results\nCALL calculate_avg_salary_by_dept();\n\n```\n\n**Uswer defined function on columns**\n\n```sql\n-- Create a function to calculate the square of a number\nCREATE OR REPLACE FUNCTION calculate_square(input_num FLOAT)\n  RETURNS FLOAT\n  LANGUAGE SQL\n  AS '\n    SELECT input_num * input_num;\n  ';\n\n-- Use the function on a column in a query\nSELECT employee_name, calculate_square(salary) AS salary_squared\nFROM employees;\n\n```\n\nYou can also change the function language\n\n```sql\n-- Create a Python function to calculate the factorial of a number\nCREATE OR REPLACE FUNCTION calculate_factorial(input_num INT)\n  RETURNS FLOAT\n  LANGUAGE PYTHON\n  AS '\n    def factorial(n):\n        if n == 0:\n            return 1\n        else:\n            return n * factorial(n-1)\n    \n    return factorial(input_num)\n  ';\n\n-- Use the Python function in a query\nSELECT employee_name, calculate_factorial(employee_id) AS emp_id_factorial\nFROM employees;\n\n\n-- Use the Python function in a query\nSELECT employee_name, calculate_factorial(employee_id) AS emp_id_factorial\nFROM employees;\n\n+--------------+------------------+\n| employee_name | emp_id_factorial |\n+--------------+------------------+\n| John         | 5040             |\n| Jane         | 3628800          |\n| Bob          | 479001600        |\n| Alice        | 2432902008176640000 |\n+--------------+------------------+\n\n\n```","tags":["snowflakes"]},{"grab":"## Dynamic Tables and Views\n\ndynamic table is important if yu want to have a temporary table for a query result that only lastes for a given section.\n\nview is close to dynamic table, but the views table is permanent","views":"e.g dynamic table\n\n```sql\n\n-- Create a dynamic table\nCREATE OR REPLACE TEMPORARY TABLE my_dynamic_table AS\nSELECT * FROM my_permanent_table WHERE some_condition;\n\n-- Query the dynamic table\nSELECT * FROM my_dynamic_table WHERE another_condition;\n\n-- Drop the dynamic table when no longer needed (end of session)\nDROP TABLE my_dynamic_table;\n\n```\n\n\nview\n\n```sql\n-- Create a view\nCREATE OR REPLACE VIEW my_view AS\nSELECT column1, column2\nFROM my_permanent_table\nWHERE some_condition;\n\n-- Query the view\nSELECT * FROM my_view;\n\n\n```","tags":["snowflakes"]},{"grab":"## Dynamic masking and ROw-level security (RLS)\n\nDynamic masking involve masking some part of the data column to prevent sensitive data. e.g displaying datata to customer care and you don't want to show the full customer card number.\n\nRLS, imagine you have a table that has multiple tenant. at each time one of the tenant want to view or access such table we want to only display the tenant rows without showing other tenants data.","views":"### Dynamic data masking\n\n1 create a table with sensitive data\n\n```sql\nCREATE OR REPLACE TABLE customer_data (\n    customer_id INT,\n    customer_name STRING,\n    ssn STRING\n);\n\n```\n\n2. Define a masking  policy on column `ssn`\n```sql\nCREATE MASKING POLICY ssn_masking_policy AS (val STRING) RETURNS STRING ->\n  CASE\n    WHEN CURRENT_ROLE() IN ('ADMIN', 'HR') THEN val -- Admins and HR see actual data.\n    ELSE 'XXX-XX-' || RIGHT(val, 4) -- Mask SSN for others.\n  END;\n\n```\n3. Apply masking policy to the table\n```sql\nALTER TABLE customer_data MODIFY COLUMN ssn SET MASKING POLICY ssn_masking_policy;\n\n```\n\nfetch data\n\n```sql\nSELECT * FROM customer_data;\n\ncustomer_id | customer_name | ssn\n---------------------------------\n1           | John Doe      | XXX-XX-1234\n2           | Jane Smith    | XXX-XX-5678\n\n\n```\n\n### RLS\n\ncreate a table with tenant-specific data\n\n```sql\nCREATE OR REPLACE TABLE orders (\n    order_id INT,\n    tenant_id INT,\n    order_date DATE,\n    total_amount FLOAT\n);\n\n```\n\ndefine a security policy for RLS that restircts acccess based on the `tenant_id`\n\n```sql\nCREATE SECURITY POLICY tenant_access_policy\n  ADD FILTER (tenant_id = CURRENT_ROLE()); -- Filter data based on user's role (tenant_id).\n\n-- Associate the security policy with the 'orders' table.\nALTER TABLE orders SET SECURITY POLICY tenant_access_policy;\n\n```\n\ngrant roles for users or tenants\n\n```sql\n-- Grant 'Tenant A' role to a user.\nGRANT ROLE \"Tenant A\" TO USER user1;\n\n-- Grant 'Tenant B' role to another user.\nGRANT ROLE \"Tenant B\" TO USER user2;\n\n\n```\n","tags":["snowflakes"]}]