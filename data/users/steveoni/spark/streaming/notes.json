[{"grab":"\n\n**foreachBatch()**\n\n```py\n hostAddr = \"<ip address>\"\nkeyspaceName = \"<keyspace>\"\ntableName = \"<tableName>\"\nspark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\ndef writeCountsToCassandra(updatedCountsDF, batchId):\n # Use Cassandra batch data source to write the updated counts\n (updatedCountsDF\n .write\n .format(\"org.apache.spark.sql.cassandra\")\n .mode(\"append\")\n .options(table=tableName, keyspace=keyspaceName)\n .save())\n\nstreamingQuery = (counts\n .writeStream\n .foreachBatch(writeCountsToCassandra)\n .outputMode(\"update\")\n .option(\"checkpointLocation\", checkpointDir)\n .start()) \n```\n ","views":"**foreachBatch()** takes in function that accept Dataframe output and batchid for each microbatch output","tags":["streaming"]},{"grab":"```py\n def writeCountsToMultipleLocations(updatedCountsDF, batchId):\n updatedCountsDF.persist()\n updatedCountsDF.write.format(...).save() # Location 1\n updatedCountsDF.write.format(...).save() # Location 2\n updatedCountsDF.unpersist() \n```\n ","views":"Each attempt to write might casuses the output data to be recomputed. including rereading of data. To prevent this we need to persist the output dataframe  by caching \n\nusing **outputDf.persist()** and **outputDf.unpersist()**\n","tags":["streaming"]},{"grab":"```py\ndef process_row(row):\n # Write row to storage\n pass\nquery = streamingDF.writeStream.foreach(process_row).start()\n# Variation 2: Using the ForeachWriter class\nclass ForeachWriter:\n def open(self, partitionId, epochId):\n # Open connection to data store\n # Return True if write should continue\n # This method is optional in Python\n # If not specified, the write will continue automatically\n return True\n def process(self, row):\n # Write string to data store using opened connection\n # This method is NOT optional in Python\n pass\n def close(self, error):\n # Close the connection. This method is optional in Python\n pass\nresultDF.writeStream.foreach(ForeachWriter()).start()\n```\n","views":"if not option to use `foreachBatch()` i.e if the batch data writer does not exist for the sink you can use `foreach`","tags":["streaming"]},{"grab":"## Stateless Transformations\nAll projection operations (e.g., select(), explode(), map(), flatMap()) and selec‚Äê\ntion operations (e.g., filter(), where()) process each input record individually\nwithout needing any information from previous rows. This lack of dependence on\nprior input data makes them stateless operations.","views":"This operation does not depend on the previous batch data and do make use of `append` and `update` to add data to the sink.","tags":["streaming"]},{"grab":"## Stateful Operation","views":"For stateful operation the executor save batch data in in-memory for the next data.\n\nThis does not ensure no failure. to resolve that, each key/value state update a change log in checkpoint. it is version along with the offset range.","tags":["streaming"]},{"grab":"## StateFuL Streaming Aggregation\n\nAggregation not base on time\n\nGlobal Aggregation\n\n```py\nrunningCount = sensorReadings.groupBy().count()\n```\n\n**Grouped Aggregations**\n\n```py\nbaselineValues = sensorReadings.groupBy(\"sensorId\").mean(\"value\")\n```\n\n","views":"for global aggregation `df.groupBy().count()`\n\nwe can't run `df.count()` directly, because for static dataframes this return the aggregate immediately, whereas for stream the aggragate is meant to be calculated continously","tags":["streaming"]},{"grab":"## Aggregations with Event-Time Windows\n\n```py\nfrom pyspark.sql.functions import *\n(sensorReadings\n .groupBy(\"sensorId\", window(\"eventTime\", \"5 minute\"))\n .count())\n```\n\n```py\n# In Python\n(sensorReadings\n .groupBy(\"sensorId\", window(\"eventTime\", \"10 minute\", \"5 minute\"))\n .count())\n```","views":"while dealing with time data like sensor it best to calculate aggreagte base on the time in the data (collected by the sensor) and no base on the time the data enters the streams\n\nslidding window do have overlapped data. for the example. taked data from the input at range of 10minutes and move the window my 5 minute data.\n\n```\nFor example, the event\n[eventTime = 12:07, sensorId = id1] gets mapped to two time windows and\ntherefore two groups, (12:00-12:10, id1) and (12:05-12:15, id1). The counts for\nthese two windows are each incremented by 1\n```","tags":["streaming"]},{"grab":"## supported output modes for aggregation with time","views":"**update mode**: support all type of aggragagtion. but can not be use with append-only streaming sink.\n\n**complete mode**:  support all aggragation. but for window time aggragation, this mode will preserve all state instead of them to be cleanup if watermark is specified. this might increase the data size and memory usage\n\n**append mode**: good for append-only streaming sink. only disadvantage is that the output will be delayed by watermark duration. i.e query has to wait for trailing watermark to exceed the time window of a key feature before its aggregate can be finalized","tags":["streaming"]},{"grab":"```py\n(sensorReadings\n .withWatermark(\"eventTime\", \"10 minutes\")\n .groupBy(\"sensorId\", window(\"eventTime\", \"10 minutes\", \"5 minutes\"))\n .mean(\"value\"))\n```","views":"withWatemark is used to mark the duration before we can not accept late data again and clear out the window data state","tags":["streaming"]},{"grab":"**Stream-Static Joins**\n\n```py\n# Static DataFrame [adId: String, impressionTime: Timestamp, ...]\n# reading from your static data source\nimpressionsStatic = spark.read. ...\n# Streaming DataFrame [adId: String, clickTime: Timestamp, ...]\n# reading from your streaming source\nclicksStream = spark.readStream. ...\n\n# In Python\nmatched = clicksStream.join(impressionsStatic, \"adId\")\n\n#or\nmatched = clicksStream.join(impressionsStatic, \"adId\", \"leftOuter\")\n\n\n```","views":"stream - static join are stateless\n\nit support three types of join\n\n* inner join\n* Left outer join when the left side is a streaming DataFrame\n* Right outer join when the right side is a streaming DataFrame\n\nTo prevent the static data to be read repeatedly, we can cache the static DataFrame","tags":["streaming"]},{"grab":"**Stream-Stream Join**\n\nInner join\n\n```py\nimpressionsWithWatermark = (impressions\n .selectExpr(\"adId AS impressionAdId\", \"impressionTime\")\n .withWatermark(\"impressionTime\", \"2 hours\"))\nclicksWithWatermark = (clicks\n .selectExpr(\"adId AS clickAdId\", \"clickTime\")\n .withWatermark(\"clickTime\", \"3 hours\"))\n# Inner join with time range conditions\n(impressionsWithWatermark.join(clicksWithWatermark,\n expr(\"\"\"\n clickAdId = impressionAdId AND\n clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\")))\n```","views":"The code you provided appears to be using Apache Spark Structured Streaming, a real-time data processing framework, to perform an inner join between two streaming DataFrames (`impressions` and `clicks`) using watermarking and a time range condition. Watermarking is used to handle late data arriving in a stream, and the time range condition is used to filter and join records where the `clickTime` falls within 1 hour of the `impressionTime`.\n\nHere's an explanation with dummy data:\n\nSuppose you have two streams of data, `impressions` and `clicks`, and you want to join them based on certain conditions.\n\n**Sample Data for Impressions Stream:**\n\n| impressionAdId | impressionTime       |\n|----------------|----------------------|\n| 1              | 2023-08-01 12:00:00  |\n| 2              | 2023-08-01 14:00:00  |\n| 3              | 2023-08-02 10:00:00  |\n| 4              | 2023-08-02 15:30:00  |\n| 5              | 2023-08-03 08:45:00  |\n\n**Sample Data for Clicks Stream:**\n\n| clickAdId | clickTime            |\n|-----------|----------------------|\n| 1         | 2023-08-01 12:30:00  |\n| 2         | 2023-08-01 14:45:00  |\n| 3         | 2023-08-02 11:15:00  |\n| 4         | 2023-08-02 16:30:00  |\n| 5         | 2023-08-03 09:30:00  |\n\nNow, let's break down the code and its execution:\n\n1. **Watermarking**:\n   - `impressions` and `clicks` DataFrames are created from streaming sources.\n   - Watermarking is applied to both DataFrames using the `.withWatermark` method. It specifies a threshold (2 hours for `impressions` and 3 hours for `clicks`) that defines how late data can be before it is considered for processing.\n\n2. **Join Operation**:\n   - The DataFrames with watermarking (`impressionsWithWatermark` and `clicksWithWatermark`) are joined using an inner join.\n   - The join condition is defined using the `expr` function. It checks if:\n     - `clickAdId` from `clicksWithWatermark` matches `impressionAdId` from `impressionsWithWatermark`, and\n     - `clickTime` from `clicksWithWatermark` falls within 1 hour of `impressionTime` from `impressionsWithWatermark`.\n\nThe result of this operation is a DataFrame containing rows where the `clickTime` falls within 1 hour of the corresponding `impressionTime`. In other words, it captures clicks that occurred shortly after impressions within the specified time window.\n\nIn our dummy data example, the result of this join operation would include rows where the `clickTime` is within 1 hour after the `impressionTime`, resulting in the following joined DataFrame:\n\n**Result of Join Operation:**\n\n| impressionAdId | impressionTime       | clickAdId | clickTime            |\n|----------------|----------------------|-----------|----------------------|\n| 1              | 2023-08-01 12:00:00  | 1         | 2023-08-01 12:30:00  |\n| 2              | 2023-08-01 14:00:00  | 2         | 2023-08-01 14:45:00  |\n| 3              | 2023-08-02 10:00:00  | 3         | 2023-08-02 11:15:00  |\n| 4              | 2023-08-02 15:30:00  | 4         | 2023-08-02 16:30:00  |\n| 5              | 2023-08-03 08:45:00  | 5         | 2023-08-03 09:30:00  |\n\nThese are the rows where clicks occurred within 1 hour of the corresponding impressions, as specified by the join condition.","tags":["streaming"]},{"grab":"outer joins with wartermarking\n\n```py\n(impressionsWithWatermark.join(clicksWithWatermark,\n expr(\"\"\"\n clickAdId = impressionAdId AND\n clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\"),\n \"leftOuter\")) \n```","views":"Unlike with inner joins, the watermark delay and event-time constraints are not\noptional for outer joins. This is because for generating the NULL results, the\nengine must know when an event is not going to match with anything else in the\nfuture. For correct outer join results and state cleanup, the watermarking and\nevent-time constraints must be specified.\n\n","tags":["streaming"]},{"grab":"Arbitrary Stateful Computations\nMany use cases require more complicated logic than the SQL operations we have dis‚Äê\ncussed up to now. For example, say you want to track the statuses (e.g., signed in,\nbusy, idle) of users by tracking their activities (e.g., clicks) in real time. To build this\nstream processing pipeline, you will have to track each user‚Äôs activity history as a state\nwith arbitrary data structure, and continuously apply arbitrarily complex changes on\nthe data structure based on the user‚Äôs actions.","views":"checkout  mapGroupsWithState() and flatMapGroupsWithState from https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf","tags":["streaming"]}]